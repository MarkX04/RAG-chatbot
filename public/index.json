[
{
	"uri": "/5-setupbackend/5.1-builddocker/",
	"title": "Build Docker from Local",
	"tags": [],
	"description": "",
	"content": " Change the path to the backend: cd backend. Create a .env file inside the rag_v1 folder.  The .env file contains the keys required to call prompts to large language models.\n\rRun the command docker build -t rag-chatbot-backend . to build the image.  Note: The build might fail if you haven\u0026rsquo;t installed C++ Build Tools. You can download them from Build Tools. Just select the C++ Build Tools option.\n\rMake sure to create a folder named chroma and another folder data. Put a sample PDF file inside the data folder. You can download sample files from sample_data.\nImportant Note: The PDF file must be in pure text format. If it\u0026rsquo;s image-based, the extraction may not work properly.\n\rStart compressing the Docker image rag-chatbot-backend into a .tar.gz file to transfer to another machine: docker save rag-chatbot-backend | gzip \u0026gt; rag-chatbot-backend.tar.gz  The gzip command is not available in PowerShell. Use Git Bash to run the command instead.\n\rThis command might take a little while to complete.\n\rTransfer the rag-chatbot-backend.tar.gz file from your local machine to the EC2 instance\u0026rsquo;s home directory using SSH, with the PEM file for authentication: scp -i ~/.ssh/keyfile.pem rag-chatbot-backend.tar.gz ubuntu@EC2-IP:~/\n(Remember to replace ~/.ssh/keyfile.pem with the actual path to your downloaded key pair and EC2-IP with the actual public IP of your EC2 instance)  Type yes if prompted with Are you sure you want to continue connecting\u0026hellip;. Your typing won’t be shown on screen.\n\rWait for the file to finish uploading to EC2. The upload time depends on your internet speed.\nSSH into your Ubuntu EC2 instance using the PEM file for authentication: ssh -i ~/.ssh/keyfile.pem ubuntu@EC2-IP\n(Remember to replace ~/.ssh/keyfile.pem and EC2-IP with the correct values)  Once connected to EC2:  Install Docker on Ubuntu EC2 Add user to the docker group Load the Docker image Run the container    sudo apt update\rsudo apt install docker.io -y\rsudo usermod -aG docker ubuntu\rnewgrp docker\rgunzip -c rag-chatbot-backend.tar.gz | docker load\rdocker run -d --name rag-chatbot-backend -p 8000:8000 rag-chatbot-backend\rCheck if the backend is running by visiting: http://\u0026lt;ec2-ip\u0026gt;:8000/docs   Monitor the backend logs in the terminal with: docker logs -f rag-chatbot-backend\n  Go back to the Front End URL you created earlier.\n  Check if you see the green text RAG Ready. That means everything is working properly.\n\r"
},
{
	"uri": "/4-setupfrondend_s3/4.1-awsconfigure/",
	"title": "Configure AWS",
	"tags": [],
	"description": "",
	"content": "Step-by-step AWS CLI Configuration  Clone the GitHub project chatbot-rag to your local machine.   Switch to the deploy branch:\n Run the command in your terminal: git checkout deploy    Enter the Access Keys that you downloaded earlier:\n Run: aws configure AWS Access Key ID: From the downloaded CSV file AWS Secret Access Key: From the downloaded CSV file Default region: us-east-1 Default output format: json    "
},
{
	"uri": "/2-prerequiste/2.1-bedrock/",
	"title": "Enabling Large Language and Embedding Models",
	"tags": [],
	"description": "",
	"content": "Enabling Large Language and Embedding Models  Log in to the AWS Console  Search for AWS Bedrock Select Amazon Bedrock    In this workshop, we will be using the Virginia United States region (us-east-1)\n\r In the Amazon Bedrock interface, scroll down in the Configure and learn section and select Model access\n  Click Modify model access\n  In this workshop, we will use the Claude 3.5 Sonnet model from Anthropic and the Embedding English model from Cohere as examples. You can choose other models if you prefer.\n\rSelect Claude 3.5 Sonnet and Embedding English, then click Next  Review the changes and click Submit   Once the services have been enabled and the IAM role or IAM user has been created, you can use the Access Key and Secret Key to send prompts to the Large Language Model (LLM) via code — for example, using AWS’s boto3 SDK.\nimport boto3 import json # Create a client for Bedrock bedrock = boto3.client( service_name=\u0026#34;bedrock-runtime\u0026#34;, region_name=\u0026#34;us-east-1\u0026#34;, # replace with your region aws_access_key_id=\u0026#34;YOUR_ACCESS_KEY_ID\u0026#34;, aws_secret_access_key=\u0026#34;YOUR_SECRET_ACCESS_KEY\u0026#34;, ) # Example: send a prompt to Claude response = bedrock.invoke_model( modelId=\u0026#34;anthropic.claude-v2\u0026#34;, contentType=\u0026#34;application/json\u0026#34;, accept=\u0026#34;application/json\u0026#34;, body=json.dumps({ \u0026#34;prompt\u0026#34;: \u0026#34;Please explain the difference between AI and Machine Learning.\u0026#34;, \u0026#34;max_tokens_to_sample\u0026#34;: 300, \u0026#34;temperature\u0026#34;: 0.7 }) ) result = response[\u0026#39;body\u0026#39;].read().decode(\u0026#39;utf-8\u0026#39;) print(result) \rIn practice, the arguments aws_access_key_id and aws_secret_access_key will not appear directly in the code. They will be stored in a .env file and loaded when needed.\n\r"
},
{
	"uri": "/",
	"title": "Retrieval-Augmented Generation highlight",
	"tags": [],
	"description": "",
	"content": "Working with Retrieval-Augmented Generation with Evidence Highlighting Feature Overview In this workshop, we will build a chatbox web application with RAG functionality along with an evidence highlighting feature directly on a PDF file. We will also explore the basics of large language models, the mechanism and principles of a RAG chatbox, and the algorithm for the evidence highlighting feature.\nContent  Theory Preparation Steps Create EC2 Initialize Front End Set Up Back End Delete Resources  "
},
{
	"uri": "/1-introduce/",
	"title": "Theory",
	"tags": [],
	"description": "",
	"content": "Retrieval-Augmented Generation chatbox with highlighted evidence functionality is an application that requires knowledge in several areas, especially in Large Language Models (LLMs). This application not only integrates the ability to retrieve information from external databases or knowledge bases, but also utilizes powerful generative models to produce contextually relevant, accurate responses tailored to user needs.\nA key highlight of the system is the highlighted evidence feature, which enables users to easily identify which parts of the source document were used by the model to generate its response. This feature enhances the system’s transparency and verifiability, thereby improving trust and user experience.\nThe following topics will be introduced in the workshop:\n Knowledge of Large Language Models (LLM) How to convert PDF documents into a Vector Database (vectordb) How Retrieval-Augmented Generation (RAG) works Highlighted evidence algorithm  "
},
{
	"uri": "/1-introduce/1.1-llm/",
	"title": "Theory of Large Language Models",
	"tags": [],
	"description": "",
	"content": "Large Language Models, also known as LLMs, are extremely large deep learning models pre-trained on massive datasets. The core transformer is a set of neural networks with an encoder and a decoder capable of self-attention. The encoder and decoder extract meaning from a text sequence and understand the relationships between words and phrases in it.\nThe Transformer architecture is the foundational core of most current large language models (LLMs) such as GPT, BERT, or LLaMA. Transformers work based on the self-attention mechanism, allowing the model to understand and relate words in a sentence regardless of the distance between them. This architecture consists of many stacked layers, each comprising two main components: multi-head self-attention and feed-forward neural networks. Input data is encoded into vectors through embeddings, then passed through the Transformer layers to produce output as a sequence of vectors representing semantics. Transformers can be designed as encoder-only (e.g., BERT), decoder-only (e.g., GPT), or encoder-decoder (e.g., T5), depending on the model’s objective.\nLarge language models are generative models\n\rLLMs are huge—extremely huge. They can consider billions of parameters and have a wide range of potential uses. Below are some examples:\n OpenAI\u0026rsquo;s GPT-3 model has 175 billion parameters. Its cousin, ChatGPT, can identify patterns from data and generate natural, readable outputs. Although the size of Claude 2 is unknown, it can accept up to 100,000 tokens per prompt, meaning it can read hundreds of pages of technical documentation or even an entire book. AI21 Labs' Jurassic-1 model has 178 billion parameters and a token vocabulary of 250,000 components with similar conversational capabilities. Cohere\u0026rsquo;s Command model offers similar capabilities and supports over 100 different languages. LightOn’s Paradigm platform provides foundation models with features claimed to outperform GPT-3. All of these LLMs come with APIs that allow developers to create unique generative AI applications.  LLMs have many practical applications.   Ad Copywriting\nIn addition to GPT-3 and ChatGPT, Claude, Llama 2, Cohere Command, and Jurassic can all generate native ad copy. AI21\u0026rsquo;s Wordspice suggests modifications to original sentences to improve style and tone.\n  Knowledge-based Q\u0026amp;A\nOften referred to as knowledge-intensive natural language processing (KI-NLP), this technique refers to LLMs capable of answering specific questions based on information stored in digital repositories. One example is answering general knowledge questions in AI21 Studio’s playground.\n  Text Classification\nLLMs can classify text with similar meanings or sentiments by clustering. Use cases include measuring customer sentiment, identifying relationships between texts, and document retrieval.\n  Code Generation\nLLMs are proficient at generating code from natural language prompts. For example, Amazon CodeWhisperer and OpenAI’s Codex, used in GitHub Copilot, can write code in Python, JavaScript, Ruby, and other languages. Other use cases include generating SQL queries, writing shell commands, and designing websites. Learn more about AI code generation.\n  Text Generation\nSimilar to code generation, text generation can complete unfinished sentences, write product documentation, or, like Alexa Create, generate short children’s stories.\n  "
},
{
	"uri": "/4-setupfrondend_s3/4.2-deployfrondend/",
	"title": "Deploy Frontend to Amazon S3",
	"tags": [],
	"description": "",
	"content": "\rThis section requires Node.js to be installed on your machine.\n\rSteps to Deploy the Frontend on Amazon S3   First, think of a unique name for your website. This will be used as the S3 bucket name (i.e., the path to your static website). Choose a name that’s less likely to be taken, for example:\n fcj-rag-chatbot-frontend fcj-rag-chatbot\netc.    Go back to your EC2 Instance and copy the Public IP address.\n  Create a .env.production file in the project root with the following content:  # Production environment variables for AWS deployment VITE_API_URL=http://IP-EC2:8000/api\rVITE_APP_NAME=RAG Chatbot\rVITE_APP_VERSION=1.0.0\rVITE_ENVIRONMENT=production\rVITE_DEBUG_MODE=false\r# AWS Configuration\rVITE_AWS_REGION=us-east-1\rVITE_S3_BUCKET=ten-website\rVITE_CLOUDFRONT_DOMAIN=your-cloudfront-domain.cloudfront.net\r# API Configuration\rVITE_API_TIMEOUT=30000\rVITE_ENABLE_ANALYTICS=true\rVITE_ENABLE_ERROR_REPORTING=true\r# Feature Flags\rVITE_ENABLE_PDF_HIGHLIGHTING=true\rVITE_ENABLE_DOCUMENT_UPLOAD=true\rVITE_MAX_FILE_SIZE=10485760\rThen adjust these two values:\n In IP-EC2: enter the public IP address you copied earlier. In VITE_S3_BUCKET: enter the name of the website bucket you chose.  In the terminal, run npm install, then run npm build.  In the terminal, run aws s3 mb s3://ten-website.  Make sure the ten-website matches the name of VITE_S3_BUCKET you set above.\n\r"
},
{
	"uri": "/2-prerequiste/2.2-gemini/",
	"title": "Enabling the Gemini Large Language Model",
	"tags": [],
	"description": "",
	"content": "Introduction to the Gemini Large Language Model In this workshop, we will introduce another model that can be used for this application: Gemini.\nUsing the Gemini LLM on Google Cloud offers several notable benefits, especially suitable for beginners or those experimenting with LLMs:\n Simple and Easy-to-Integrate Prompting   Official Python SDK: vertexai Sending a prompt is as simple as calling a Python function JSON or object-based structure that\u0026rsquo;s easy to read and parse  chat.send_message(\u0026#34;Please write a simple chatbot using Streamlit.\u0026#34;) Gemini 1.5 Pro is Currently Free    Google is currently offering access to Gemini 1.5 Pro completely free of charge via:\n Google AI Studio Vertex AI (via Notebook or API) This is a great opportunity to experiment with a powerful LLM at no cost.    No Billing Card Required    If you\u0026rsquo;re using a personal Google account (Gmail) to:\n Sign in to AI Studio, or Create a new Google Cloud Project, You can use Gemini for free without needing to add a billing card. Google provides a default free quota that’s sufficient for experimentation or learning. This is especially suitable for students, AI learners, or hands-on workshops.    Multimodal Support (text + image + code)    Gemini is one of the few LLMs that supports multimodal input, including:\n Text Images Code    This enables you to solve a wide range of problems, such as:\n Synthesizing and summarizing documents Explaining programming code Image recognition and analysis Building comprehensive AI-powered applications Perfect for real-world scenarios, educational use, or intelligent chatbot development     Enabling the Gemini Large Language Model  Visit Google AI Studio and sign in  Go to the Google Cloud Console, and click on Console  In the project selection dropdown, choose to create a new project  For the Location, select No organization.\nThen click Create\nReturn to Google AI Studio. Click on Get API Key, then select Create API Key  Select the project you just created. Then proceed to create the Gemini API key.  Scroll down and you will see the API Key you just created. Be sure to keep this key secure.  Be sure to keep your API keys secure, even if you are using a free billing plan.\n\r"
},
{
	"uri": "/2-prerequiste/",
	"title": "Preparation Steps",
	"tags": [],
	"description": "",
	"content": "\rYou need to have an AWS account ready.\n\rPreparation Overview To prepare for the workshop, you need to perform some basic configuration steps on AWS, including:\n Enabling Large Language Models (LLMs) Enabling Embedding Models Creating and configuring an IAM Role so that services have access permissions  "
},
{
	"uri": "/1-introduce/1.2-embedding/",
	"title": "Theory of Embeddings for Storage",
	"tags": [],
	"description": "",
	"content": "Embeddings are numerical representations of real-world objects that machine learning (ML) and artificial intelligence (AI) systems use to understand complex knowledge domains like humans do. For example, computational algorithms understand that the difference between 2 and 3 is 1, showing a closer relationship between 2 and 3 than between 2 and 100. However, real-world data involves more complex relationships. For instance, bird nests and lion dens are analogous pairs, while day-night are opposites. Embeddings transform real-world objects into complex mathematical representations that capture the inherent attributes and relationships in real-world data. The entire process is automated, with AI systems generating embeddings during training and using them when needed to perform new tasks.\nEmbeddings allow deep learning models to understand real-world data domains more efficiently. They simplify how real-world data is represented while preserving semantic and syntactic relationships. This enables ML algorithms to extract and process complex data types and powers advanced AI applications. The following sections describe several key aspects:\n Dimensionality reduction Training large language models Building creative applications  Embeddings enable new deep learning and AI applications. Different embedding techniques used in neural network architectures support the development, training, and deployment of accurate AI models across various fields and use cases. For example:\n With image embedding technology, engineers can build high-accuracy computer vision applications for object detection, image recognition, and other vision-related tasks. With word embeddings, natural language processing software can better understand the context and relationships between words. Graph embeddings extract and classify relevant information from connected nodes to support network analysis.  Computer vision models, AI chatbots, and AI recommendation systems all use embeddings to perform complex tasks that mimic human intelligence.\nData embeddings transform raw data into continuous values that ML models can interpret. Typically, ML models use one-hot encoding to map categorical variables into learnable forms. This encoding method splits each category into rows and columns and assigns them binary values. Consider the following produce categories and their prices.\nOne-hot encoding expands values into 0s and 1s without providing information that helps the model relate different objects. For example, the model cannot detect similarities between apples and oranges, even though they are both fruits, nor can it distinguish between oranges and carrots as fruit and vegetables. As the number of categories increases, one-hot encoding results in sparse variables with many empty values, consuming a lot of memory space.\nEmbeddings vectorize objects into a lower-dimensional space by representing similarities between objects as numerical values. Neural network embeddings ensure dimensionality remains manageable even with extensive input features. Input features are the attributes of specific objects that the ML algorithm is tasked with analyzing. Dimensionality reduction allows embeddings to retain the information that ML models use to identify similarities and differences from input data. Data scientists can also visualize embeddings in two-dimensional space to better understand the relationships between distributed objects.\nAmazon Bedrock is a fully managed service that offers a variety of high-performing foundation models (FMs) from leading AI companies, along with a broad set of features for building generative AI applications. Amazon Nova is the new generation of state-of-the-art (SOTA) foundation models, offering advanced intelligence and industry-leading price/performance. These are powerful, versatile models designed to support a wide range of use cases. You can use them out-of-the-box or customize them with your own data.\nTitan Embeddings is a machine learning (ML) foundation model that converts text into numerical representations. The Titan Embeddings model supports text retrieval, clustering, and semantic comparison. The input text limit is 8K tokens, and the maximum output vector length is 1536.\nML teams can also use Amazon SageMaker to generate embeddings. Amazon SageMaker is a hub where you can build, train, and deploy ML models in a secure and scalable environment. It provides an embedding technique called Object2Vec, allowing engineers to vectorize multi-dimensional data into a multi-dimensional space. You can use the learned embeddings to compute object relationships for downstream tasks such as classification and regression.\n"
},
{
	"uri": "/5-setupbackend/5.2-use/",
	"title": "Using RAG",
	"tags": [],
	"description": "",
	"content": "Example images of how to use the application. "
},
{
	"uri": "/4-setupfrondend_s3/4.3-configres3/",
	"title": "Configure Amazon S3",
	"tags": [],
	"description": "",
	"content": "Configure Your Amazon S3 Bucket for Static Website Hosting   In the AWS Console, search for the S3 service.\n  In the left menu, select General purpose buckets. Click on the bucket you created earlier — this is your your-website-name.\n  Go to the Permissions tab.  Under Block public access, click Edit.  Uncheck the box Block all public access, then click Save changes.  Scroll down to the Bucket Policy section and click Edit.  Paste the following policy into the editor (replace your-website-name with your actual bucket name):  { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::your-website-name/*\u0026#34; } ] } Then, go to the Properties tab. Scroll down to Static website hosting and click Edit.  Under Static website hosting, select Enable. For both Index document and Error document, enter index.html, then click Save.  Back in your terminal, upload the dist folder (which was generated from building the frontend) to the S3 bucket. Make sure the bucket name matches what you used earlier:  aws s3 sync dist/ s3://your-website-name Access http://ten-website.s3-website-us-east-1.amazonaws.com. (Replace with your actual bucket name) to check the Front End.  Since the Back End has not been deployed yet, you will see a red message in the bottom right corner saying Backend Offline.\n\r"
},
{
	"uri": "/3-setupec2/",
	"title": "Create EC2 Instance",
	"tags": [],
	"description": "",
	"content": "In this step, we will create an EC2 instance for the RAG (Retrieval-Augmented Generation) application.\n Search for EC2 using the AWS Console search bar.  In the left sidebar, click on Instances, then choose Launch Instance.  Name your EC2 instance.  Under Amazon Machine Image, select:  Ubuntu Server 22.04 LTS (HVM), SSD Volume Type    In the Instance Type section, choose t3.small.\nFor Key pair (login), either select an existing key or create a new one.  Make sure to download the .pem key file to your local machine.\n\rFor VPC, select Default.  Modify the Inbound Rules to allow access:  Type: SSH, Source Type: Anywhere, Port Range: 22 Type: Custom TCP, Source Type: Anywhere, Port Range: 8000    Under Storage, choose 24 GiB with volume type gp2.  Finally, click Launch Instance to start your EC2 instance.  "
},
{
	"uri": "/2-prerequiste/2.3-iam/",
	"title": "Create IAM Role with Required Permissions",
	"tags": [],
	"description": "",
	"content": "Creating and Assigning IAM Role Permissions To follow best practices, we should create a dedicated user account and assign permissions using IAM roles.\n\r In the AWS Console search bar, search for IAM and select it. Click on Create User.  Enter a Username. Under Provide user access to the AWS Management Console, choose I want to create an IAM user, then click Next.  Under the Set Permissions section, choose Attach policies directly.  Select the following 3 policies:  AdministratorAccess AmazonS3FullAccess AmazonEC2FullAccess    Click Next, review the selected permissions to confirm, then click Create user.  Download the .csv file containing the username and password details.  Go back to the newly created user and select Create access key.  Select Command Line Interface (CLI). Check the box I understand the above recommendation and want to proceed to create an access key. Then click Next, and Next again.  Download the .csv file containing the access key credentials and store it securely.  Never share the contents of the downloaded .csv files with others.\n\rThese access key files are used to authenticate when accessing Amazon Bedrock\u0026rsquo;s Embedding and LLM services.\n\r"
},
{
	"uri": "/1-introduce/1.3-rag/",
	"title": "Theory of Retrieval-Augmented Generation",
	"tags": [],
	"description": "",
	"content": "Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model by referencing an authoritative external knowledge base beyond its training data before generating a response. A large language model (LLM) is trained on massive datasets and uses billions of parameters to produce initial outputs for tasks such as question answering, language translation, and sentence completion. RAG extends the already powerful capabilities of an LLM to specific domains or an organization’s internal knowledge base — all without retraining the model. This is a cost-effective way to enhance LLM output so that it remains relevant, accurate, and useful in a wide range of contexts.\nLLMs are a key artificial intelligence (AI) technology that powers intelligent chatbots and other natural language processing (NLP) applications. The goal is to create bots that can answer user questions in various contexts by cross-referencing authoritative sources of knowledge. Unfortunately, the nature of LLM technology introduces unpredictability in responses. Additionally, LLM training data is static and introduces a cutoff date for existing knowledge.\nCommon challenges with LLMs include:\n Providing misleading information when no correct answer is known. Returning outdated or generic information when users expect specific, current answers. Generating responses from non-authoritative sources. Producing inaccurate answers due to terminology confusion, where different training sources use the same terms to refer to different things.  A large language model can be thought of as an overly enthusiastic new employee who refuses to update information about current events but still answers every question with absolute confidence. Unfortunately, such behavior negatively impacts user trust — and is not something you want your chatbot to emulate!\nWhat are the benefits of Retrieval-Augmented Generation?  Cost-effective implementation Up-to-date information Improved user trust Greater developer control  How does Retrieval-Augmented Generation work? Without RAG, an LLM takes the user\u0026rsquo;s input and generates a response based on the information it was trained on — what it already \u0026ldquo;knows.\u0026rdquo; With RAG, an information retrieval component is introduced to use the user’s input to fetch relevant information from a new data source. The user query and the retrieved information are both passed to the LLM. The LLM then uses this new knowledge along with its training data to generate a better response. The following sections provide an overview of the process.\n Generate external data Retrieve relevant information Augment LLM prompts Update external data  The following diagram illustrates the conceptual process of using RAG with an LLM.\n"
},
{
	"uri": "/4-setupfrondend_s3/",
	"title": "Initialize Front End",
	"tags": [],
	"description": "",
	"content": "In this step, we will perform the initial setup for the Front End before deployment.\nYou can refer to the lab Getting Started With Amazon S3 for more detailed instructions.\n\r"
},
{
	"uri": "/1-introduce/1.4-algorithm_highlighted/",
	"title": "PDF Text Highlighting Algorithm using PyMuPDF Library",
	"tags": [],
	"description": "",
	"content": "Introduction When working with PDF documents in Python, highlighting text is a common operation — especially in applications involving information extraction, marking important data, or displaying search results. In this article, we’ll explore how to highlight text in a PDF file using the PyMuPDF (fitz) library.\nHow to Highlight Text in PDF with PyMuPDF Find text positions using .search_for() First, you need to open the PDF document and find the text fragments to highlight using the .search_for() function of a Page object. This function returns a list of Rect objects corresponding to the positions of the found text.\nimport fitz # PyMuPDF # Open the PDF file doc = fitz.open(\u0026#34;example.pdf\u0026#34;) # Iterate through each page for page in doc: # Search for the positions matching the search string text_instances = page.search_for(\u0026#34;highlight me\u0026#34;) # Highlight each found region for inst in text_instances: highlight = page.add_highlight_annot(inst) # Save the result doc.save(\u0026#34;highlighted.pdf\u0026#34;, garbage=4, deflate=True) \rLarge language models can return answers in a JSON format.\n\rExample:\n[\r{{ \u0026quot;chunk_id\u0026quot;: ..., \u0026quot;highlight_text\u0026quot;: \u0026quot;...\u0026quot; }},\r...\r]\r\rBased on this capability, when sending a prompt to any large language model, you can ask it to return its answer along with a JSON format for the text that should be highlighted. This means the highlight functionality heavily depends on the language model\u0026rsquo;s output.\n\rHowever, the nature of large language models is generative. They do not always guarantee the exact output format or text (e.g., in this application, they might return text that differs slightly from the exact original text in the PDF). Since .search_for() requires an exact match in characters and formatting, relying solely on the language model means the highlighting will only work for a subset of prompts — not consistently.\n\r One solution to this issue is to write an algorithm that handles cases where the language model returns slightly altered text compared to the original in the PDF.  Solution: Fuzzy Matching for Inexact Highlighting We will use a fuzzy matching algorithm to find approximate text matches on a PDF page, then create highlights for the matched regions.\npartial_highlight() Algorithm Below is the partial_highlight() function, which helps find approximate text and apply highlights, even when exact matches aren’t found:\ndef partial_highlight(pdf_path, output_path, text_to_highlight, page_number, file_exist, threshold=90): doc = fitz.open(pdf_path) page = doc[page_number] page_text = page.get_text() # Clean the input text target_text = text_to_highlight.replace(\u0026#34;\\\\n\u0026#34;, \u0026#34;\\n\u0026#34;).strip() # Find approximate matches using fuzzy matching spans = find_spans_fuzzy(page, target_text, threshold) if not spans: print(\u0026#34;--------------Failed to find highlight partial!\u0026#34;) return else: for span in spans: highlight = page.add_highlight_annot(span) highlight.update() print(\u0026#34;------------------Partial highlight checking---------------\u0026#34;) # Save the PDF to disk if file_exist: temp_output = output_path + \u0026#34;.temp.pdf\u0026#34; doc.save(temp_output, garbage=4, deflate=True, clean=True) doc.close() shutil.move(temp_output, output_path) else: doc.save(output_path, garbage=4, deflate=True, clean=True) doc.close() Helper Function: find_spans_fuzzy() This function loops through all words on a PDF page and performs fuzzy matching using a sliding window approach to locate regions with similar text:\nfrom fuzzywuzzy import fuzz def find_spans_fuzzy(page, target, threshold=90, buffer=10): spans = [] words = page.get_text(\u0026#34;words\u0026#34;) # Each word is (x0, y0, x1, y1, word, block_no, line_no, word_no) words.sort(key=lambda w: (w[1], w[0])) # Sort by position: top-to-bottom, left-to-right word_texts = [w[4] for w in words] target_len = len(target.split()) max_window = min(len(words), target_len + buffer) for i in range(len(words) - max_window + 1): for window in range(target_len, max_window + 1): window_words = word_texts[i:i+window] window_text = \u0026#34; \u0026#34;.join(window_words) score = fuzz.partial_ratio(window_text, target) if score \u0026gt;= threshold: rects = [fitz.Rect(w[:4]) for w in words[i:i+window]] span = rects[0] for r in rects[1:]: span |= r # union all rectangles spans.append(span) break # Stop checking longer windows at this position once a match is found return spans "
},
{
	"uri": "/5-setupbackend/",
	"title": "Set Up BackEnd",
	"tags": [],
	"description": "",
	"content": "In this section, we will perform the final steps to set up the Back End on the EC2 instance.\n"
},
{
	"uri": "/6-eraseresource/",
	"title": "Delete Resources",
	"tags": [],
	"description": "",
	"content": "This section helps you quickly delete the resources used on AWS to avoid unexpected charges.  Go back to the EC2 service and select the EC2 instance you used for the application. Click Stop Instance and then Terminate Instance.  Then go to the S3 service and select the correct bucket you created. First, empty the bucket.  Type permanently delete and click Empty.  Then go back to the general bucket view and click Delete.  Enter the exact website name you set earlier to confirm you want to delete. Then click Delete bucket.  Go to the IAM service to delete the IAM ROLE.  Deactivate the access key. Be sure to deactivate it before fully deleting the IAM.  Type Confirm and click Delete.  "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]