[{"uri":"https://markx04.github.io/RAG-chatbot/vi/5-setupbackend/5.1-builddocker/","title":"Build Docker từ local","tags":[],"description":"","content":" Thay đổi đường dẫn đến backend: cd backend . Tạo 1 file .env trong folder rag_v1. File .env chứa các key để gọi promt đến các mô hình ngôn ngữ lớn.\nNhập lệnh docker build -t rag-chatbot-backend . để build images. Lưu ý có khả năng khi build sẽ bị fail do chưa cài C++ Build Tools. Các bạn có thể tự tải C++ Build Tools từ Build Tools. Chọn option C++ Build Tools là được.\nLưu ý các bạn cũng phải tạo 1 thư mục tên chroma và 1 thư mục data. Và hãy kiếm cái file pdf bỏ vào folder data. Các bạn có thể lấy các file từ sample_data. Lưu ý đặc biệt: file pdf phải là file pdf thuần text. Nếu file pdf image thì có khả năng trích xuất ảnh ko được tốt.\nBắt đầy nén image Docker rag-chatbot-backend thành một file .tar.gz để chuyển qua máy khác. docker save rag-chatbot-backend | gzip \u0026gt; rag-chatbot-backend.tar.gz Lệnh gzip không có trong powershell. Nên ta bật git bash để nhập lệnh đó.\nChờ lệnh này hơi lâu 1 chút.\nGửi file rag-chatbot-backend.tar.gz từ máy lên EC2 ở thư mục home của user ubuntu thông qua SSH, dùng file PEM để xác thực. scp -i ~/.ssh/tenfile.pem rag-chatbot-backend.tar.gz ubuntu@EC2-IP:~/ (Nhớ cái chỗ ~/.ssh/tenfile.pem ghi đúng địa chỉ file key pair bạn đã tải và EC2-IP cũng ghi đúng địa chỉ public IP EC2 instance) Bạn nhớ nhập yes vào Are you sure you want to\u0026hellip;. Chữ đánh máy sẽ không hiển thị.\nSau đó bạn sẽ chờ file được tải lên ec2. Tốc độ tùy thuộc vào mạng nhà của bạn\nĐăng nhập vào máy chủ EC2 chạy Ubuntu, sử dụng file PEM để xác thực thay vì nhập mật khẩu. ssh -i ~/.ssh/tenfile.pem ubuntu@EC2-IP (Nhớ cái chỗ ~/.ssh/tenfile.pem ghi đúng địa chỉ file key pair bạn đã tải và EC2-IP cũng ghi đúng địa chỉ public IP EC2 instance) Sau khi ssh vào EC2: Cài Docker trên Ubuntu EC2 Thêm user vào nhóm docker Load Docker image Chạy container sudo apt update\rsudo apt install docker.io -y\rsudo usermod -aG docker ubuntu\rnewgrp docker\rgunzip -c rag-chatbot-backend.tar.gz | docker load\rdocker run -d --name rag-chatbot-backend -p 8000:8000 rag-chatbot-backend Kiểm tra backend có đang hoạt động hay không bằng cách vào: http://\u0026lt;ec2-ip\u0026gt;:8000/docs Kiểm tra trong terminal: docker logs -f rag-chatbot-backend\nQuay lại cái đường dẫn Frond End bạn đã tạo lúc đầu.\nCheck xem thấy chữ RAG Ready màu xanh lá cây là ok.\n"},{"uri":"https://markx04.github.io/RAG-chatbot/vi/4-setupfrondend_s3/4.1-awsconfigure/","title":"Cấu hình AWS","tags":[],"description":"","content":" Clone Project Github chatbot-rag về local Đổi qua branch deploy.\nNhập terminal git checkout deploy. Nhập các Key đã tải lúc trước.\nNhập terminal aws configure AWS Access Key ID: lấy từ file csv đã tải. AWS Secret Access Key: lấy từ file csv đã tải. Default region: us-east-1 Default output format: json "},{"uri":"https://markx04.github.io/RAG-chatbot/vi/2-prerequiste/2.1-bedrock/","title":"Kích hoạt model ngôn ngữ lớn và embedding","tags":[],"description":"","content":"Kích hoạt model ngôn ngữ lớn và embedding Đăng nhập vào AWS console Tìm AWS Bedrock Chọn Amazon Bedrock Trong bài workshop này chúng ta sẽ sử dụng Region Virgina United States (us-east-1)\nTrong giao diện Amazon Bedrock kéo thanh menu bên phải xuống trong mục Configure and learn và chọn Model access\nChọn Modify model access\nTrong bài workshop này chúng ta sẽ sử dụng Model Claude 3.5 Sonnet của Anthropic và Embedding English của Cohere để làm ví dụ. Bạn có thể hoàn toàn chọn những model khác thay thế.\nChọn vào Claude 3.5 Sonnet và Embedding English. Rồi bấm Next Kiểm tra lại các thay đổi, rồi bấm Submit Sau khi đã enable dịch vụ và tạo IAM role hoặc IAM user, bạn có thể sử dụng Access Key và Secret Key để gửi prompt đến mô hình ngôn ngữ lớn (LLM) thông qua code — ví dụ với SDK boto3 của AWS. import boto3 # Tạo client kết nối đến Bedrock bedrock = boto3.client( service_name=\u0026#34;bedrock-runtime\u0026#34;, region_name=\u0026#34;us-east-1\u0026#34;, # thay bằng region phù hợp aws_access_key_id=\u0026#34;YOUR_ACCESS_KEY_ID\u0026#34;, aws_secret_access_key=\u0026#34;YOUR_SECRET_ACCESS_KEY\u0026#34;, ) # Ví dụ gửi prompt tới Claude response = bedrock.invoke_model( modelId=\u0026#34;anthropic.claude-v2\u0026#34;, contentType=\u0026#34;application/json\u0026#34;, accept=\u0026#34;application/json\u0026#34;, body=json.dumps({ \u0026#34;prompt\u0026#34;: \u0026#34;Làm ơn giải thích sự khác nhau giữa AI và Machine Learning.\u0026#34;, \u0026#34;max_tokens_to_sample\u0026#34;: 300, \u0026#34;temperature\u0026#34;: 0.7 }) ) result = response[\u0026#39;body\u0026#39;].read().decode(\u0026#39;utf-8\u0026#39;) print(result) Trong code thực hành sẽ ko có các argument aws_access_key_id và aws_secret_access_key vì ta sẽ bỏ chúng vào 1 file .env và sẽ load lên sử dụng.\n"},{"uri":"https://markx04.github.io/RAG-chatbot/vi/1-introduce/","title":"Lý thuyết","tags":[],"description":"","content":"Retrieval-Augmented Generation chatbox cùng với tính năng tô sáng bằng chứng là một ứng dụng cần phải có kiến thức về nhiều mảng đặc biệt là các kiến thức về Mô hình ngôn ngữ lớn. Ứng dụng này không chỉ kết hợp khả năng truy xuất thông tin từ cơ sở dữ liệu hoặc kho tri thức bên ngoài, mà còn sử dụng các mô hình sinh văn bản mạnh mẽ để tạo ra câu trả lời có ngữ cảnh, chính xác và sát với nhu cầu người dùng.\nĐiểm nổi bật của hệ thống là tính năng tô sáng bằng chứng (highlighted evidence), cho phép người dùng dễ dàng nhận biết được phần thông tin nào trong nguồn tài liệu đã được mô hình sử dụng để đưa ra câu trả lời. Tính năng này tăng cường tính minh bạch và khả năng kiểm chứng của hệ thống, giúp nâng cao độ tin cậy và trải nghiệm người dùng.\nCác kiến thức tiếp theo sau đây sẽ được giới thiệu trong workshop:\nKiến thức về mô hình ngôn ngữ lớn (LLM) Cách thức chuyển tài liệu PDF ra một Vector Database (vectordb) Cách thức hoạt động của RAG (Retrieval-Augmented Generation) Thuật toán tô sáng bằng chứng (highlighted evidence algorithm) "},{"uri":"https://markx04.github.io/RAG-chatbot/vi/1-introduce/1.1-llm/","title":"Lý thuyết về mô hình ngôn ngữ lớn","tags":[],"description":"","content":"Mô hình ngôn ngữ lớn, còn gọi là LLM, là các mô hình học sâu rất lớn, được đào tạo trước dựa trên một lượng dữ liệu khổng lồ. Bộ chuyển hóa cơ bản là tập hợp các mạng nơ-ron có một bộ mã hóa và một bộ giải mã với khả năng tự tập trung. Bộ mã hóa và bộ giải mã trích xuất ý nghĩa từ một chuỗi văn bản và hiểu mối quan hệ giữa các từ và cụm từ trong đó.\nKiến trúc Transformer là nền tảng cốt lõi của hầu hết các mô hình ngôn ngữ lớn (LLM) hiện nay như GPT, BERT hay LLaMA. Transformer hoạt động dựa trên cơ chế self-attention, cho phép mô hình hiểu và liên kết các từ trong một câu bất kể khoảng cách giữa chúng. Kiến trúc này gồm nhiều lớp lặp lại (stacked layers), mỗi lớp bao gồm hai thành phần chính: multi-head self-attention và mạng nơ-ron feed-forward. Dữ liệu đầu vào được mã hóa thành vector thông qua embedding, sau đó truyền qua chuỗi các lớp Transformer để tạo ra đầu ra là chuỗi các vector đại diện cho ngữ nghĩa. Transformer có thể được thiết kế theo dạng encoder-only (như BERT), decoder-only (như GPT), hoặc encoder-decoder (như T5), tùy theo mục tiêu của mô hình.\nCác mô hình ngôn ngữ lớn là mô hình tạo sinh\nLLM rất lớn, vô cùng lớn. Chúng có thể xem xét hàng tỷ tham số và có nhiều cách sử dụng tiềm năng. Dưới đây là một số ví dụ:\nMô hình GPT-3 của Open AI có 175 tỷ tham số. Người anh em họ của nó là ChatGPT có thể xác định các nhiều mô hình từ dữ liệu, từ đó tạo ra kết quả tự nhiên và có thể đọc được. Mặc dù chúng tôi không biết kích thước của Claude 2, nhưng nó có thể nhận dữ liệu đầu vào lên đến 100.000 token trong mỗi lời nhắc, có nghĩa là nó có thể đọc hàng trăm trang tài liệu kỹ thuật hoặc thậm chí toàn bộ cuốn sách. Mô hình Jurassic-1 của AI21 Labs có 178 tỷ tham số và một kho từ vựng token gồm 250.000 thành phần từ cùng khả năng trò chuyện tương tự. Mô hình Command của Cohere có khả năng tương tự và có thể hoạt động trong hơn 100 ngôn ngữ khác nhau. Nền tảng Paradigm của LightOn cung cấp các mô hình nền tảng với các tính năng được công bố là vượt trội so với các tính năng của mô hình GPT-3. Tất cả các LLM này đều đi kèm với API cho phép các nhà phát triển tạo ra các ứng dụng AI tạo sinh độc đáo. LLM có rất nhiều ứng dụng thực tế. Viết quảng cáo Ngoài GPT-3 và ChatGPT, Claude, Llama 2, Cohere Command và Jurassic cũng có thể viết quảng cáo gốc. AI21 Wordspice đề xuất những thay đổi đối với câu gốc để cải thiện văn phong và giọng điệu.\nTrả lời dựa trên cơ sở kiến thức Thường được gọi là xử lý ngôn ngữ tự nhiên chuyên sâu về kiến thức (KI-NLP), kỹ thuật này đề cập đến các LLM có khả năng trả lời những câu hỏi cụ thể dựa trên thông tin được lưu trữ trong kho lưu trữ kỹ thuật số. Một ví dụ là khả năng trả lời câu hỏi về kiến thức tổng quát của sân chơi AI21 Studio.\nPhân loại văn bản LLM có thể phân loại văn bản có ý nghĩa hoặc quan điểm tương tự nhau bằng cách sử dụng cụm. Các trường hợp sử dụng bao gồm đo lường quan điểm khách hàng, xác định mối quan hệ giữa các văn bản và tìm kiếm tài liệu.\nTạo mã LLM thành thạo trong việc tạo mã từ lời nhắc ngôn ngữ tự nhiên. Ví dụ: Amazon CodeWhisperer và codex của Open AI được sử dụng trong GitHub Copilot có thể viết mã bằng Python, JavaScript, Ruby và một số ngôn ngữ lập trình khác. Các ứng dụng viết mã khác bao gồm tạo truy vấn SQL, viết lệnh shell và thiết kế trang web. Tìm hiểu thêm về tạo mã AI.\nTạo văn bản Tương tự như tạo mã, tạo văn bản có thể hoàn tất các câu không hoàn chỉnh, viết tài liệu về sản phẩm hoặc, như Alexa Create, viết một câu chuyện ngắn dành cho trẻ em.\n"},{"uri":"https://markx04.github.io/RAG-chatbot/vi/","title":"Retrieval-Augmented Generation highlight","tags":[],"description":"","content":"Làm việc với Retrieval-Augmented Generation kèm thêm tính năng tô sáng bằng chứng Tổng quan Trong bài workshop này, ta sẽ làm 1 ứng dụng chatbox trên website với chức năng RAG kèm thêm tính năng tô sáng bằng chứng trực tiếp trên file pdf. Ta cũng phải tìm hiểu các kiến thức cơ bản về mô hình ngôn ngữ lớn, cách thức và nguyên lý hoạt động của RAG chatbox và thuật toán cho tính năng tô sáng bằng chứng.\nNội dung Lý thuyết Các bước chuẩn bị Tạo EC2 Khởi tạo Frond End Thiết lập BackEnd Xóa tài nguyên "},{"uri":"https://markx04.github.io/RAG-chatbot/vi/2-prerequiste/","title":"Các bước chuẩn bị","tags":[],"description":"","content":"\rBạn cần tạo sẵn 1 account AWS.\nTổng quan chuẩn bị Để chuẩn bị cho workshop, bạn cần thực hiện một số bước cấu hình cơ bản trên AWS, bao gồm:\nKích hoạt (enable) các mô hình ngôn ngữ lớn (Large Language Models) Kích hoạt mô hình nhúng (Embedding Models) Tạo và cấu hình IAM Role để các dịch vụ có quyền truy cập "},{"uri":"https://markx04.github.io/RAG-chatbot/vi/4-setupfrondend_s3/4.2-deployfrondend/","title":"Deploy FrondEnd lên Amazon S3","tags":[],"description":"","content":"\rPhần này yêu cầu máy đã cài đặt NodeJs\nĐầu tiên ta phải suy nghĩ cái tên cho trang web. ten-website: là tên cho đường dẫn tới static web, nên dùng những tên ít phổ biến để tránh gặp lỗi, ví dụ: fcj-rag-chatbot-frontend, fcj-rag-chatbot,…\nQuay lại cái Instance EC2 và copy cái địa chỉ public IP lại.\nTạo file .env.production. Nhập vào đoạn code sau: # Production environment variables for AWS deployment VITE_API_URL=http://IP-EC2:8000/api\rVITE_APP_NAME=RAG Chatbot\rVITE_APP_VERSION=1.0.0\rVITE_ENVIRONMENT=production\rVITE_DEBUG_MODE=false\r# AWS Configuration\rVITE_AWS_REGION=us-east-1\rVITE_S3_BUCKET=ten-website\rVITE_CLOUDFRONT_DOMAIN=your-cloudfront-domain.cloudfront.net\r# API Configuration\rVITE_API_TIMEOUT=30000\rVITE_ENABLE_ANALYTICS=true\rVITE_ENABLE_ERROR_REPORTING=true\r# Feature Flags\rVITE_ENABLE_PDF_HIGHLIGHTING=true\rVITE_ENABLE_DOCUMENT_UPLOAD=true\rVITE_MAX_FILE_SIZE=10485760 Sau đó chỉnh 2 chỗ:\nChỗ IP-EC2: nhập vào địa chỉ public IP đã copy. Chỗ VITE_S3_BUCKET: cái tên website bạn đã chọn. Nhập Terminal npm install. Sau đó nhập npm build Nhập Terminal aws s3 mb s3://ten-website. Nhớ cái ten-website trùng với tên VITE_S3_BUCKE ở trên bạn đã đặt\n"},{"uri":"https://markx04.github.io/RAG-chatbot/vi/2-prerequiste/2.2-gemini/","title":"Kích hoạt model ngôn ngữ lớn Gemini","tags":[],"description":"","content":"Giới thiệu về model ngôn ngữ lớn Gemini Trong workshop này, mình sẽ giới thiệu cho các bạn biết thêm 1 cái model có thể sử dụng trong ứng dụng này là Gemini.\nViệc sử dụng mô hình ngôn ngữ Gemini trong Google Cloud có nhiều lợi ích đáng chú ý, đặc biệt phù hợp cho người mới bắt đầu hoặc những người đang thử nghiệm mô hình LLM:\nCode prompt đơn giản, dễ tích hợp SDK vertexai hỗ trợ Python chính thức Gửi prompt đơn giản như gọi hàm Python Cấu trúc JSON hoặc object dễ đọc, dễ parse chat.send_message(\u0026#34;Viết giúp mình đoạn code tạo chatbot đơn giản với Streamlit.\u0026#34;) Hiện tại Gemini 1.5 Pro đang miễn phí Google đang cho phép sử dụng mô hình Gemini 1.5 Pro hoàn toàn miễn phí thông qua:\nGoogle AI Studio Vertex AI (sử dụng qua Notebook hoặc API) Đây là cơ hội tuyệt vời để thử nghiệm một mô hình LLM mạnh mẽ mà không mất chi phí. Không cần gắn thẻ thanh toán (billing card) Nếu bạn sử dụng tài khoản Google cá nhân (Gmail) để:\nĐăng nhập vào AI Studio, hoặc Tạo Google Cloud Project mới Thì bạn có thể dùng Gemini miễn phí mà không cần thêm thẻ thanh toán. Google cung cấp quota miễn phí mặc định, đủ để thử nghiệm hoặc học tập. Điều này đặc biệt phù hợp cho sinh viên, người học AI, hoặc trong các buổi workshop thực hành. Có hỗ trợ đa modal (text + image + code) Gemini là một trong số ít mô hình ngôn ngữ lớn hỗ trợ đa modal, bao gồm:\nVăn bản (Text) Hình ảnh (Image) Mã nguồn (Code) Điều này giúp bạn giải quyết được nhiều bài toán hơn, như:\nTổng hợp và tóm tắt tài liệu Giải thích mã lập trình Nhận diện và phân tích hình ảnh Xây dựng ứng dụng AI tổng hợp Rất phù hợp với các bài toán thực tế, ứng dụng giáo dục, hay xây dựng chatbot thông minh. Kích hoạt model ngôn ngữ lớn Gemini Truy cập vào Google AI Studio và bấm đăng nhập Truy cập vào Google Cloud Console, bấm vào chữ Console Chọn vào ô project và tạo 1 project mới. Chỗ Location chọn No organization.\nXong rồi bấm Create\nQuay lại trang Google AI Studio. Bấm vào Get API Key, chọn Create API Key. Chọn đúng project bạn vừa tạo ở trên. Rồi bấm tạo API key Gemini. Kéo xuống dưới và bạn sẽ thấy Key vừa được tạo. Hãy giữ bảo mật key này. Hãy nhớ bảo mật các key này cho kĩ dù cho có đang là Free billing.\n"},{"uri":"https://markx04.github.io/RAG-chatbot/vi/1-introduce/1.2-embedding/","title":"Lý thuyết về Hàm nhúng để lưu trữ","tags":[],"description":"","content":"Hàm nhúng là các biểu diễn số của các đối tượng trong thế giới thực mà các hệ thống học máy (ML) và trí tuệ nhân tạo (AI) sử dụng để hiểu các miền kiến thức phức tạp như con người. Ví dụ, các thuật toán điện toán hiểu rằng hiệu số giữa 2 và 3 là 1, cho thấy mối quan hệ chặt chẽ giữa 2 và 3 so với 2 và 100. Tuy nhiên, dữ liệu trong thế giới thực bao gồm các mối quan hệ phức tạp hơn. Ví dụ, tổ chim và hang sư tử là các cặp tương tự, trong khi ngày-đêm là các thuật ngữ trái nghĩa. Nhúng chuyển đổi các đối tượng trong thế giới thực thành các biểu diễn toán học phức tạp nắm bắt các thuộc tính và mối quan hệ vốn có giữa dữ liệu trong thế giới thực. Toàn bộ quá trình được tự động hóa, với các hệ thống AI tự tạo nhúng trong quá trình đào tạo và sử dụng chúng khi cần để hoàn thành các nhiệm vụ mới.\nNhúng cho phép các mô hình học sâu hiểu các miền dữ liệu thực tế hiệu quả hơn. Chúng đơn giản hóa cách dữ liệu thực tế được biểu diễn trong khi vẫn giữ nguyên các mối quan hệ ngữ nghĩa và cú pháp. Điều này cho phép các thuật toán học máy trích xuất và xử lý các loại dữ liệu phức tạp, đồng thời cho phép các ứng dụng AI tiên tiến. Các phần sau đây sẽ mô tả một số yếu tố quan trọng.\nGiảm chiều dữ liệu Đào tạo các mô hình ngôn ngữ lớn Xây dựng các ứng dụng sáng tạo Nhúng cho phép các ứng dụng học sâu và trí tuệ nhân tạo (AI) mới . Các kỹ thuật nhúng khác nhau được áp dụng trong kiến trúc mạng nơ-ron cho phép phát triển, đào tạo và triển khai các mô hình AI chính xác trong nhiều lĩnh vực và ứng dụng khác nhau. Ví dụ:\nVới công nghệ nhúng hình ảnh, các kỹ sư có thể xây dựng các ứng dụng thị giác máy tính có độ chính xác cao để phát hiện đối tượng, nhận dạng hình ảnh và các tác vụ liên quan đến thị giác khác. Với việc nhúng từ, phần mềm xử lý ngôn ngữ tự nhiên có thể hiểu chính xác hơn ngữ cảnh và mối quan hệ giữa các từ. Nhúng đồ thị trích xuất và phân loại thông tin liên quan từ các nút được kết nối để hỗ trợ phân tích mạng. Các mô hình thị giác máy tính, chatbot AI và hệ thống đề xuất AI đều sử dụng nhúng để hoàn thành các nhiệm vụ phức tạp mô phỏng trí thông minh của con người.\nNhúng dữ liệu chuyển đổi dữ liệu thô thành các giá trị liên tục mà mô hình ML có thể diễn giải. Thông thường, các mô hình ML sử dụng mã hóa one-hot để ánh xạ các biến phân loại thành các dạng mà chúng có thể học được. Phương pháp mã hóa chia mỗi danh mục thành các hàng và cột và gán cho chúng các giá trị nhị phân. Hãy xem xét các danh mục nông sản sau và giá của chúng.\nMã hóa one-hot mở rộng các giá trị chiều 0 và 1 mà không cung cấp thông tin giúp mô hình liên hệ các đối tượng khác nhau. Ví dụ, mô hình không thể tìm thấy điểm tương đồng giữa táo và cam mặc dù chúng là trái cây, cũng không thể phân biệt cam và cà rốt là trái cây và rau củ. Khi danh sách càng nhiều danh mục, mã hóa sẽ dẫn đến các biến phân bố thưa thớt với nhiều giá trị rỗng, chiếm dụng rất nhiều không gian bộ nhớ.\nNhúng vector hóa các đối tượng thành không gian ít chiều bằng cách biểu diễn các điểm tương đồng giữa các đối tượng bằng các giá trị số. Nhúng mạng nơ-ron đảm bảo số chiều vẫn có thể quản lý được với các đặc trưng đầu vào mở rộng. Đặc trưng đầu vào là các đặc điểm của các đối tượng cụ thể mà thuật toán ML được giao nhiệm vụ phân tích. Giảm chiều cho phép nhúng giữ lại thông tin mà các mô hình ML sử dụng để tìm điểm tương đồng và khác biệt từ dữ liệu đầu vào. Các nhà khoa học dữ liệu cũng có thể trực quan hóa các nhúng trong không gian hai chiều để hiểu rõ hơn mối quan hệ của các đối tượng phân tán.\nAmazon Bedrock là một dịch vụ được quản lý toàn diện, cung cấp nhiều lựa chọn mô hình nền tảng (FM) hiệu suất cao từ các công ty AI hàng đầu, cùng với một bộ tính năng đa dạng để xây dựng các ứng dụng trí tuệ nhân tạo (AI) tạo sinh. Amazon Nova là thế hệ mô hình nền tảng (FM) tiên tiến (SOTA) mới, mang đến trí tuệ tiên tiến và hiệu suất giá/hiệu suất hàng đầu trong ngành. Đây là những mô hình mạnh mẽ, đa năng được xây dựng để hỗ trợ nhiều trường hợp sử dụng khác nhau. Bạn có thể sử dụng chúng theo nguyên trạng hoặc tùy chỉnh chúng với dữ liệu của riêng bạn.\nTitan Embeddings là một LLM (Lý thuyết học máy) chuyển đổi văn bản thành biểu diễn số. Mô hình Titan Embeddings hỗ trợ truy xuất văn bản, phân cụm và so sánh ngữ nghĩa. Văn bản đầu vào tối đa là 8K token và độ dài vectơ đầu ra tối đa là 1536.\nCác nhóm học máy cũng có thể sử dụng Amazon SageMaker để tạo nhúng. Amazon SageMaker là một trung tâm nơi bạn có thể xây dựng, đào tạo và triển khai các mô hình ML trong một môi trường an toàn và có thể mở rộng. Nó cung cấp một kỹ thuật nhúng gọi là Object2Vec, cho phép các kỹ sư vector hóa dữ liệu đa chiều trong không gian đa chiều. Bạn có thể sử dụng các nhúng đã học để tính toán mối quan hệ giữa các đối tượng cho các tác vụ hạ nguồn như phân loại và hồi quy.\n"},{"uri":"https://markx04.github.io/RAG-chatbot/vi/5-setupbackend/5.2-use/","title":"Sử dụng RAG","tags":[],"description":"","content":"Hình ảnh ví dụ cách dùng ứng dụng. "},{"uri":"https://markx04.github.io/RAG-chatbot/vi/4-setupfrondend_s3/4.3-configres3/","title":"Cấu hình Amazon S3","tags":[],"description":"","content":" Tìm kiếm dịch vụ S3 của AWS.\nỞ mục menu bên trái chọn General purpose buckets. Chọn vào đúng cái ten-website lúc nãy bạn đã đặt.\nChọn tab Permission. Bấm vào Edit trong mục Block public access Bấm uncheck Block all public access. Rồi chọn Save changes. Rồi tiếp tục kéo xuống phần Bucket Policy. Chọn Edit. Copy lệnh này bỏ vào(Nhớ chỗ ten-website để đúng cái tên bạn đã dùng): { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::ten-website/*\u0026#34; } ] } Sau đó chọn tiếp tab Properties. Kéo xuống dưới chỗ Static website hosting chọn Edit. Static web hosting chọn Enable. Phần Index document và Error document đều nhập index.html. Sau đó bấm Save. Quay về Terminal upload folder dist vừa build từ frontend (Nhớ là để đúng tên website bạn đã chọn): aws s3 sync dist/ s3://ten-website Truy cập http://ten-website.s3-website-us-east-1.amazonaws.com. (Thay đúng tên) để check xem Frond End. Lúc này chưa có Back End. Nên góc dưới bên phải sẽ hiện dòng chữ màu đỏ Backend Offline\n"},{"uri":"https://markx04.github.io/RAG-chatbot/vi/1-introduce/1.3-rag/","title":"Lý thuyết về Sinh văn bản truy vấn tăng cường","tags":[],"description":"","content":"Tạo tăng cường truy xuất (RAG) là quá trình tối ưu hóa đầu ra của một mô hình ngôn ngữ lớn, vì vậy nó tham khảo một cơ sở kiến thức có thẩm quyền bên ngoài các nguồn dữ liệu đào tạo của nó trước khi tạo phản hồi. Mô hình ngôn ngữ lớn (LLM) được đào tạo trên khối lượng dữ liệu khổng lồ và có sử dụng hàng tỷ tham số để tạo ra đầu ra ban đầu cho các nhiệm vụ như trả lời câu hỏi, dịch ngôn ngữ và hoàn thành câu. RAG mở rộng các khả năng vốn đã mạnh mẽ của LLM đến các miền cụ thể hoặc cơ sở kiến thức nội bộ của tổ chức, tất cả mà không cần đào tạo lại mô hình. Đây là một cách tiếp cận hiệu quả về chi phí để cải thiện đầu ra LLM, để nó vẫn phù hợp, chính xác và hữu ích trong nhiều bối cảnh khác nhau.\nLLM là một công nghệ trí tuệ nhân tạo (AI) quan trọng, hỗ trợ các chatbot thông minh và các ứng dụng xử lý ngôn ngữ tự nhiên (NLP) khác. Mục tiêu là tạo ra các bot có thể trả lời các câu hỏi của người dùng trong nhiều bối cảnh bằng cách tham chiếu chéo các nguồn kiến thức có thẩm quyền. Rất tiếc, bản chất của công nghệ LLM đưa ra sự không thể đoán trước trong các phản hồi LLM. Ngoài ra, dữ liệu đào tạo LLM là tĩnh và giới thiệu ngày giới hạn về kiến thức hiện có.\nCác thách thức đã biết của LLM bao gồm:\nTrình bày thông tin sai lệch khi nó không có câu trả lời. Trình bày thông tin lỗi thời hoặc chung chung khi người dùng mong chờ một phản hồi cụ thể, hiện tại. Tạo phản hồi từ những nguồn không có thẩm quyền. Tạo phản hồi không chính xác do nhầm lẫn thuật ngữ, trong đó các nguồn đào tạo khác nhau sử dụng cùng một thuật ngữ để nói về những điều khác nhau. Mô hình ngôn ngữ lớn như một nhân viên mới quá nhiệt tình, từ chối cập nhật thông tin về các sự kiện hiện tại nhưng sẽ luôn trả lời mọi câu hỏi với sự tự tin tuyệt đối. Rất tiếc, một thái độ như vậy ảnh hưởng tiêu cực đến sự tin tưởng của người dùng và không phải là thứ bạn muốn chatbot của mình mô phỏng!\nLợi ích của Tạo tăng cường truy xuất là gì: Thực hiện tiết kiệm chi phí Thông tin hiện tại Nâng cao niềm tin của người dùng Nhà phát triển có quyền kiểm soát tốt hơn Tạo tăng cường truy xuất hoạt động như thế nào? Nếu không có RAG, LLM nhận đầu vào của người dùng và tạo ra phản hồi dựa trên thông tin mà nó đã được đào tạo — hoặc những gì nó đã biết. Với RAG, một thành phần truy xuất thông tin được giới thiệu sử dụng đầu vào của người dùng để lấy trước thông tin từ một nguồn dữ liệu mới. Truy vấn người dùng và thông tin có liên quan đều được cung cấp cho LLM. LLM sử dụng kiến thức mới và dữ liệu đào tạo của nó để tạo ra phản hồi tốt hơn. Các phần sau đây cung cấp thông tin tổng quan về quy trình.\nTạo dữ liệu bên ngoài Truy xuất thông tin liên quan Tăng cường lời nhắc LLM Cập nhật dữ liệu bên ngoài Sơ đồ sau đây cho thấy quy trình mang tính khái niệm của việc sử dụng RAG với LLM.\n"},{"uri":"https://markx04.github.io/RAG-chatbot/vi/3-setupec2/","title":"Tạo EC2","tags":[],"description":"","content":"Trong bước này, chúng ta sẽ tạo EC2 cho ứng dụng RAG.\nTìm kiếm EC2 trên thanh tìm kiếm AWS console. Chọn vào mục Instance bên thanh bên trái. Sau đó chọn Launch Instance. Đặt tên cho EC2. Chọn hệ điều hành Ubuntu. Amazon Machine Image chọn loại Ubuntu Server 22.04 LTS (HVM), SSD Volume Type Phần Instance Type chọn t3.small. Key pair thì hoặc là chọn cái đã có hoặc là tạo mới. Lưu ý phải phải tải file .pem về local.\nVPC chọn Default. Inbound Rule chỉnh 2 thứ: Type: SSH; Source Type: Anywhere; Port Range: 22 Type: Custom TCP; Source Type: Anywhere; Port Range: 8000 Phần storage chọn 24 GiB loại gp2. Sau đó bấm Launch Instance. "},{"uri":"https://markx04.github.io/RAG-chatbot/vi/2-prerequiste/2.3-iam/","title":"Tạo IAM role với các quyền cần thiết","tags":[],"description":"","content":"Tạo và Phân quyền cho IAM Role Để tuân thủ Best Practice. Chúng ta nên tạo acc và phân quyền IAM Role\nVào mục tìm kiếm trên console của AWS. Chọn IAM. Bấm vào Create User. Đặt tên Username. Chọn mục Provide user access to the AWS Management Console. Ở mục User type chọn I want to create an IAM user. Rồi chọn Next. Mục Set Permission chọn Attach policies directly. Trong các Policy chọn 3 options: Search Policy: AdministratorAccess Search Policy: AmazonS3FullAccess Search Policy: AmazonEC2FullAccess Sau khi bấm Next kiểm tra lại các quyền đã chọn coi đúng chưa. Sau đó bấm vào Create user Tải xuống file csv chứa các thông tin về user và password Bấm lại vào User vừa tạo và Create access key. Chọn Command Line Interface (CLI). Nhớ bấm tick chỗ I understand the above recommendation and want to proceed to create an access key. Chọn Next, và Next tiếp. Download file csv access key của IAM về và lưu trữ. Tuyệt đối không chia sẻ các thông tin về các file csv đã download.\nCác file access key này dùng để sử dụng mỗi khi sử dụng embedding và LLM từ Amazon Bedrock\n"},{"uri":"https://markx04.github.io/RAG-chatbot/vi/4-setupfrondend_s3/","title":"Khởi tạo Frond End","tags":[],"description":"","content":"Tiếp theo trong phần này chúng ta sẽ làm các thao tác để tạo Frond End trước khi deploy.\nCác bạn có thể tham khảo bài lab Khởi Đầu Với Amazon S3\n"},{"uri":"https://markx04.github.io/RAG-chatbot/vi/1-introduce/1.4-algorithm_highlighted/","title":"Thuật toán highlight text pdf sử dụng thư viện pymupdf","tags":[],"description":"","content":"Giới thiệu Khi xử lý tài liệu PDF trong Python, việc tô sáng (highlight) văn bản là một thao tác phổ biến, đặc biệt trong các ứng dụng trích xuất thông tin, đánh dấu dữ liệu quan trọng, hoặc hiển thị kết quả tìm kiếm. Trong bài viết này, chúng ta sẽ tìm hiểu cách thực hiện highlight text trong file PDF bằng cách sử dụng thư viện PyMuPDF (fitz).\nCách highlight văn bản trong PDF với PyMuPDF Tìm vị trí văn bản bằng .search_for() Đầu tiên, bạn cần mở tài liệu PDF và tìm các đoạn văn bản muốn highlight bằng hàm .search_for() của đối tượng Page. Hàm này sẽ trả về danh sách các Rect tương ứng với vị trí của văn bản được tìm thấy.\nimport fitz # PyMuPDF # Mở file PDF doc = fitz.open(\u0026#34;example.pdf\u0026#34;) # Duyệt từng trang for page in doc: # Tìm các vị trí khớp với chuỗi tìm kiếm text_instances = page.search_for(\u0026#34;highlight me\u0026#34;) # Tô sáng từng vùng tìm được for inst in text_instances: highlight = page.add_highlight_annot(inst) # Lưu file kết quả doc.save(\u0026#34;highlighted.pdf\u0026#34;, garbage=4, deflate=True) Các mô hình ngôn ngữ lớn có thể đưa các câu trả lời theo 1 format json.\nVí dụ:\n[\r{{ \u0026#34;chunk_id\u0026#34;: ..., \u0026#34;highlight_text\u0026#34;: \u0026#34;...\u0026#34; }},\r...\r] Dựa vào tính chất này. Khi đặt 1 promt gửi cho ngôn ngữ lớn bất kì. Yêu cầu trong promt rằng hãy phân tích và gửi lại cùng lúc câu trả lời kèm với format json cho chỗ cần highlight. Nên bản chất tính năng highlight cần chỉ chủ yếu là nhờ mô hình ngôn ngữ lớn giải đáp.\nNhưng bản chất của ngôn ngữ lớn là 1 mô hình tạo sinh. Là 1 mô hình ko phải lúc nào cũng đảm bảo output ra thứ mà mình muốn (Ví dụ cụ thể trong ứng dụng này là ta yêu cầu nó gửi lại chính xác text cần được highlight thì sẽ có lúc nó gửi 1 text bị khác đi 1 ký tự hoặc định dạng mà ta đã gửi) mà hàm .search_for() cần chính xác định dạng và ký tự. Suy ra nếu chỉ dựa vào ngôn ngữ lớn thì ứng dụng này sẽ chỉ highlight được khoảng 1 phần các lượt promt chứ không phải lúc này cũng vậy.\nMột giải pháp cho bất cập này là ta sẽ viết 1 giải thuật để giải quyết các trường hợp ngôn ngữ lớn trả về output bị khác đi 1 chút so với chính xác trong pdf. Giải pháp highlight không chính xác bằng fuzzy matching Chúng ta sẽ sử dụng một giải thuật fuzzy matching để tìm những đoạn văn bản gần đúng trong trang PDF, sau đó tạo highlight tương ứng với vùng tìm được.\nGiải thuật partial_highlight() Dưới đây là hàm partial_highlight() giúp tìm kiếm văn bản gần đúng và thực hiện highlight, kể cả khi không tìm thấy khớp tuyệt đối:\ndef partial_highlight(pdf_path, output_path, text_to_highlight, page_number, file_exist, threshold=90): doc = fitz.open(pdf_path) page = doc[page_number] page_text = page.get_text() # Làm sạch text input target_text = text_to_highlight.replace(\u0026#34;\\\\n\u0026#34;, \u0026#34;\\n\u0026#34;).strip() # Tìm vùng gần đúng bằng fuzzy matching spans = find_spans_fuzzy(page, target_text, threshold) if not spans: print(\u0026#34;--------------Failed to find highlight partial!\u0026#34;) return else: for span in spans: highlight = page.add_highlight_annot(span) highlight.update() print(\u0026#34;------------------Partial highlight checking---------------\u0026#34;) # Ghi file PDF ra đĩa if file_exist: temp_output = output_path + \u0026#34;.temp.pdf\u0026#34; doc.save(temp_output, garbage=4, deflate=True, clean=True) doc.close() shutil.move(temp_output, output_path) else: doc.save(output_path, garbage=4, deflate=True, clean=True) doc.close() Hàm phụ trợ find_spans_fuzzy() Hàm này duyệt qua toàn bộ các từ (words) trên trang PDF và thực hiện so khớp mờ trên từng cửa sổ trượt để tìm vùng có văn bản tương tự:\nfrom fuzzywuzzy import fuzz def find_spans_fuzzy(page, target, threshold=90, buffer=10): spans = [] words = page.get_text(\u0026#34;words\u0026#34;) # Mỗi từ là (x0, y0, x1, y1, word, block_no, line_no, word_no) words.sort(key=lambda w: (w[1], w[0])) # Sắp xếp theo vị trí: trên xuống, trái sang phải word_texts = [w[4] for w in words] target_len = len(target.split()) max_window = min(len(words), target_len + buffer) for i in range(len(words) - max_window + 1): for window in range(target_len, max_window + 1): window_words = word_texts[i:i+window] window_text = \u0026#34; \u0026#34;.join(window_words) score = fuzz.partial_ratio(window_text, target) if score \u0026gt;= threshold: rects = [fitz.Rect(w[:4]) for w in words[i:i+window]] span = rects[0] for r in rects[1:]: span |= r # union các vùng chữ lại spans.append(span) break # Dừng kiểm tra dài hơn nếu đã match tại vị trí này return spans "},{"uri":"https://markx04.github.io/RAG-chatbot/vi/5-setupbackend/","title":"Thiết lập BackEnd","tags":[],"description":"","content":"Tiếp theo trong phần này chúng ta sẽ làm các thao tác cuối cùng với Back end trên EC2.\n"},{"uri":"https://markx04.github.io/RAG-chatbot/vi/6-eraseresource/","title":"Xóa tài nguyên","tags":[],"description":"","content":"Phần này ta sẽ xóa nhanh các tài nguyên đã sử dụng trên AWS để tránh bị tính tiền quá mong muốn Truy cập lại dịch vụ EC2 và chọn lại đúng cái EC2 instance đã dùng cho ứng dụng. Stop Instance và sau đó Terminate Instance. Truy cập tiếp qua dịch vụ S3 và chọn đúng cái bucket đã tạo. Đầu tiên phải empty cái bucket đó. Nhập permanently delete và bấm Empty Sau đó quay lại chỗ bucket general bấm vào Delete. Nhập vào đúng tên website đã đặt để chắc chắn là bạn muốn delete. Sau đó bấm Delete bucket. Truy cập vào dịch vụ IAM để xóa IAM ROLE. Deactivate access key. Hãy nhớ deactivate trước khi xóa hoàn toàn IAM. Nhập Confirm và bấm Delete "},{"uri":"https://markx04.github.io/RAG-chatbot/vi/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://markx04.github.io/RAG-chatbot/vi/tags/","title":"Tags","tags":[],"description":"","content":""}]